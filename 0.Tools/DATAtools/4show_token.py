import os
import argparse
import sentencepiece as spm
from tqdm import tqdm

def load_tokenizer(model_path):
    """
    åŠ è½½ SentencePiece æ¨¡å‹
    :param model_path: .model æ–‡ä»¶è·¯å¾„
    :return: SentencePieceProcessor å®ä¾‹
    """
    if not os.path.exists(model_path):
        raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
    
    sp = spm.SentencePieceProcessor()
    sp.load(model_path)
    print(f"âœ… æˆåŠŸåŠ è½½ tokenizer: {model_path}")
    print(f"   è¯è¡¨å¤§å°: {sp.get_piece_size()}")
    return sp

def tokenize_file(input_file, output_file, sp, output_ids=False):
    """
    å¯¹è¾“å…¥æ–‡æœ¬æ–‡ä»¶é€è¡Œåˆ†è¯ï¼Œå¹¶å†™å…¥è¾“å‡ºæ–‡ä»¶
    :param input_file: è¾“å…¥ .txt æ–‡ä»¶è·¯å¾„ï¼ˆä¸€è¡Œä¸€å¥ï¼‰
    :param output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
    :param sp: å·²åŠ è½½çš„ SentencePieceProcessor
    :param output_ids: æ˜¯å¦è¾“å‡º token IDï¼ˆTrueï¼‰è¿˜æ˜¯ token å­—ç¬¦ä¸²ï¼ˆFalseï¼‰
    """
    if not os.path.exists(input_file):
        raise FileNotFoundError(f"è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {input_file}")

    # è‡ªåŠ¨åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(os.path.dirname(output_file), exist_ok=True)

    with open(input_file, 'r', encoding='utf-8') as fin, \
         open(output_file, 'w', encoding='utf-8') as fout:

        lines = [line.strip() for line in fin if line.strip()]
        
        for line in tqdm(lines, desc="ğŸ”¤ åˆ†è¯è¿›åº¦"):
            if output_ids:
                # è¾“å‡º token ID åºåˆ—ï¼ˆç©ºæ ¼åˆ†éš”ï¼‰
                ids = sp.encode(line)
                fout.write(' '.join(map(str, ids)) + '\n')
            else:
                # è¾“å‡º token å­—ç¬¦ä¸²ï¼ˆå¦‚ 'â–I â–love â–NLP'ï¼‰
                pieces = sp.encode_as_pieces(line)
                fout.write(' '.join(pieces) + '\n')

    print(f"ğŸ‰ åˆ†è¯å®Œæˆï¼ç»“æœå·²ä¿å­˜è‡³: {output_file}")

def main():
    parser = argparse.ArgumentParser(description="ä½¿ç”¨ SentencePiece æ¨¡å‹å¯¹ TXT æ–‡ä»¶è¿›è¡Œæ‰¹é‡åˆ†è¯")
    parser.add_argument('--input', type=str, default=r"0.Tools\DATAtools\result\example_en.txt", help='è¾“å…¥æ–‡æœ¬æ–‡ä»¶è·¯å¾„ (.txt)')
    parser.add_argument('--model', type=str,default=r"0.Tools\DATAtools\result\spm_en.model", help='SentencePiece æ¨¡å‹è·¯å¾„ (.model)')
    parser.add_argument('--output', type=str, default=None, help='è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆé»˜è®¤è‡ªåŠ¨ç”Ÿæˆï¼‰')
    parser.add_argument('--ids', action='store_true', help='æ˜¯å¦è¾“å‡º token ID è€Œé token å­—ç¬¦ä¸²')

    args = parser.parse_args()

    # è®¾ç½®é»˜è®¤è¾“å‡ºè·¯å¾„
    if args.output is None:
        base_name = os.path.splitext(args.input)[0]
        suffix = "_ids.txt" if args.ids else "_tokens.txt"
        args.output = base_name + suffix

    try:
        # 1. åŠ è½½ tokenizer
        sp = load_tokenizer(args.model)

        # 2. æ‰§è¡Œåˆ†è¯
        tokenize_file(args.input, args.output, sp, output_ids=args.ids)

    except Exception as e:
        print(f"âŒ å‘ç”Ÿé”™è¯¯: {e}")

if __name__ == '__main__':
    main()